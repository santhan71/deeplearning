{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DA_PA2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhan71/deeplearning/blob/master/DA_PA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHLjBrkmT1Q_",
        "colab_type": "text"
      },
      "source": [
        "                                                                    Deep learning CIFAR10 ASSSIGNMENT\n",
        "                                                                             EE19S001 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iw5zJmDUIGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MHKg8hTjsK",
        "colab_type": "text"
      },
      "source": [
        "     The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.\n",
        "\n",
        "      Computer algorithms for recognizing objects in photos often learn by example. CIFAR-10 is a set of images that can be used to teach a computer how to recognize objects. Since the images in CIFAR-10 are low-resolution (32x32), this dataset can allow researchers to quickly try different algorithms to see what works. Various kinds of convolutional neural networks tend to be the best at recognizing the images in CIFAR-10.\n",
        "\n",
        "      CIFAR-10 is a labeled subset of the 80 million tiny images dataset. When the dataset was created, students were paid to label all of the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy_c8e_VUZF3",
        "colab_type": "text"
      },
      "source": [
        "      Importing all the  libraries required for the programme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOPzn2hUWYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02jNt-uDdImy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from itertools import chain \n",
        "import scipy.spatial.distance as dist\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwvy5TC1U6HG",
        "colab_type": "text"
      },
      "source": [
        "     Downloading the CIFAR10 train dataset and test dataset and loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Gdd66udoZI",
        "colab_type": "code",
        "outputId": "11d46beb-4d44-4a17-cb3e-e528f71350a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transforms.ToTensor())\n",
        "\n",
        "testset =torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform= transforms.ToTensor())\n",
        "trainloader = torch.utils.data.DataLoader(trainset)\n",
        "testloader = torch.utils.data.DataLoader(testset)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:02, 65677189.89it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBdChYd0VaXu",
        "colab_type": "text"
      },
      "source": [
        "      Cropping the given train and test dataset to 1000 and 100  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4bvl6n8eDgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_list = []\n",
        "for i in range(1000):\n",
        "    train_list.append(trainset[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhFlUMCaeM4i",
        "colab_type": "code",
        "outputId": "1d282354-0631-4551-92b0-1a5758909c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_list[0])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMvYfV4eJrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = []\n",
        "for i in range(1000):\n",
        "    train_labels.append(trainset[i][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8oYCCOWftv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_list = []\n",
        "for i in range(100):\n",
        "    test_list.append(testset[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktB6l-nbje6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX6tHoA0gzP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = []\n",
        "for i in range(100):\n",
        "    test_labels.append(testset[i][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKQuLUIvXr7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FkgJxc5hBXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_list = []\n",
        "for i in range(1000):\n",
        "  f=train_list[i][0].view(1,-1)[0]\n",
        "  hist_list.append(f)\n",
        "  \n",
        "\n",
        "data_train = []\n",
        "\n",
        "for i in hist_list:\n",
        "  temp_list = []\n",
        "  temp_list.append(np.histogram(i[0:1024], [j for j in range(256)])[0])\n",
        "  temp_list.append(np.histogram(i[1024:2048], [j for j in range(256)])[0])\n",
        "  temp_list.append(np.histogram(i[2048:3072], [j for j in range(256)])[0])\n",
        "  data_train.append(torch.tensor(temp_list))\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy18QnlSiV-f",
        "colab_type": "code",
        "outputId": "7f44105f-347d-4eb0-b731-d3c5af1461a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(data_train[0][0]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMdV64bfZsh3",
        "colab_type": "text"
      },
      "source": [
        "          converting the given 32*32*3 image as a histogram equaliser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeBNF_wnzCwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_list1 = []\n",
        "for i in range(100):\n",
        "  r=test_list[i][0].view(1,-1)[0]\n",
        "  hist_list1.append(r)\n",
        "  \n",
        "\n",
        "data_test = []\n",
        "\n",
        "for i in hist_list1:\n",
        "  temp1_list = []\n",
        "  temp1_list.append(np.histogram(i[0:1024], [j for j in range(256)])[0])\n",
        "  temp1_list.append(np.histogram(i[1024:2048], [j for j in range(256)])[0])\n",
        "  temp1_list.append(np.histogram(i[2048:3072], [j for j in range(256)])[0])\n",
        "  data_test.append(torch.tensor(temp1_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQd0soTLZ_MN",
        "colab_type": "text"
      },
      "source": [
        "               k-NN algortihm  logic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-1vYYeP0pXW",
        "colab_type": "code",
        "outputId": "c1cdc53f-f064-4aca-d9bc-3b4652dcc0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "list_result1 = []\n",
        "for i in range(100):\n",
        "  temp_list = []\n",
        "  for j in range(1000):\n",
        "    d1=abs(((data_test[i])-(data_train[j]))**2)\n",
        "    dist = torch.sqrt(torch.sum(d1).double())\n",
        "    temp_list.append(dist)\n",
        "  list_result1.append(np.array(temp_list).argsort()[:1])\n",
        "#list_result = np.array(list_result)\n",
        "\n",
        "print(list_result1)\n",
        "print(len(list_result1))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([500]), array([499]), array([499]), array([499]), array([499]), array([499]), array([211]), array([500]), array([499]), array([499]), array([499]), array([968]), array([499]), array([792]), array([67]), array([499]), array([926]), array([499]), array([828]), array([499]), array([926]), array([366]), array([499]), array([500]), array([499]), array([499]), array([499]), array([746]), array([44]), array([499]), array([499]), array([499]), array([499]), array([934]), array([499]), array([499]), array([499]), array([207]), array([278]), array([499]), array([499]), array([499]), array([499]), array([780]), array([499]), array([118]), array([499]), array([499]), array([499]), array([499]), array([541]), array([499]), array([499]), array([792]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([203]), array([499]), array([499]), array([499]), array([539]), array([585]), array([650]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([499]), array([290]), array([499]), array([500]), array([499]), array([499]), array([265]), array([499]), array([499]), array([499]), array([499]), array([499]), array([211]), array([499]), array([771]), array([499]), array([499]), array([499]), array([499]), array([499]), array([816]), array([500])]\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKZHS4nS-I29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_list1 = []\n",
        "for i in range(100):\n",
        "  x_list1.append([train_labels[z] for z in list_result1[i]])\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiiRb73ZLY3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_list1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yfBPzaKLa2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIbpm42IL7zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list = []\n",
        "for i in range(100):\n",
        "  x_list1[i]\n",
        "  final_list.append(Counter(x_list1[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QogtM8khR6_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(final_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zBAT3FtaHue",
        "colab_type": "text"
      },
      "source": [
        "          getting the labels of the images from their Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMqUG-YQLyec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "971dbc57-3dc0-461f-8c85-c1e949cacbbb"
      },
      "source": [
        "predictions = []\n",
        "for item in final_list:\n",
        "  predictions.append(list(item.keys())[0])\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 3, 9, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 7, 1, 7, 7, 7, 7, 4, 7, 7, 7, 3, 9, 7, 7, 7, 7, 4, 7, 9, 7, 7, 7, 7, 9, 7, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 7, 7, 7, 2, 8, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 5, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T87WfZGYaP5z",
        "colab_type": "text"
      },
      "source": [
        "                 Getting the efficiency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZyUGRmfO20B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=0\n",
        "for i in range(100):\n",
        "  if(predictions[i]== test_labels[i]):\n",
        "    c=c+1\n",
        "  \n",
        "print(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V56JYo47ZaFf",
        "colab_type": "text"
      },
      "source": [
        "              craeting the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MBDALujPOCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "\n",
        "\n",
        "class_names = ['0','1','2','3','4','5','6','7','8','9']\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = [classes[i] for i in unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(test_labels,predictions, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec2autbXb5tC",
        "colab_type": "text"
      },
      "source": [
        "                                   k_nn algorithm for K=3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRdG5n3kagQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_result2 = []\n",
        "for i in range(100):\n",
        "  temp_list = []\n",
        "  for j in range(1000):\n",
        "    d1=abs(((data_test[i])-(data_train[j]))**2)\n",
        "    dist = torch.sqrt(torch.sum(d1).double())\n",
        "    temp_list.append(dist)\n",
        "  list_result2.append(np.array(temp_list).argsort()[:3])\n",
        "#list_result = np.array(list_result)\n",
        "\n",
        "print(list_result2)\n",
        "print(len(list_result2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY67zLXYavwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_list2 = []\n",
        "for i in range(100):\n",
        "  x_list2.append([train_labels[z] for z in list_result2[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJrX4ffPayMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_list2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zopQggkVbBIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list1 = []\n",
        "for i in range(100):\n",
        "  x_list2[i]\n",
        "  final_list1.append(Counter(x_list2[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61Zr9dT0bFue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(final_list1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSOShXN9bN3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions1 = []\n",
        "for item in final_list1:\n",
        "  predictions1.append(list(item.keys())[0])\n",
        "\n",
        "print(predictions1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax2lmiFvbW_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d=0\n",
        "for i in range(100):\n",
        "  if(predictions1[i]== test_labels[i]):\n",
        "    d=d+1\n",
        "  \n",
        "print(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYsve8IabsWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "\n",
        "\n",
        "class_names = ['0','1','2','3','4','5','6','7','8','9']\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = [classes[i] for i in unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(test_labels,predictions1, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUEmCEDXcZtm",
        "colab_type": "text"
      },
      "source": [
        "         same procedurecan be followed for K=5 and the accuarcy is 16 perecnt "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydeY3BTdcmpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6fe086da-a59d-4155-d3d3-a1b91aa14754"
      },
      "source": [
        "list_result3 = []\n",
        "for i in range(100):\n",
        "  temp_list = []\n",
        "  for j in range(1000):\n",
        "    d1=abs(((data_test[i])-(data_train[j]))**2)\n",
        "    dist = torch.sqrt(torch.sum(d1).double())\n",
        "    temp_list.append(dist)\n",
        "  list_result3.append(np.array(temp_list).argsort()[:5])\n",
        "#list_result = np.array(list_result)\n",
        "\n",
        "print(list_result3)\n",
        "print(len(list_result3))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([500, 876, 880, 607, 390]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([211, 321, 293, 332, 519]), array([500, 876, 880, 607, 390]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([968, 198, 561,  31, 773]), array([499, 595, 596, 597, 598]), array([792, 563,   0, 198,  56]), array([ 67, 828, 213, 155, 604]), array([499, 595, 596, 597, 598]), array([926, 809, 917, 675, 576]), array([499, 595, 596, 597, 598]), array([828, 140, 696, 893, 610]), array([499, 595, 596, 597, 598]), array([926, 809, 917, 675, 576]), array([366, 624, 325, 205, 734]), array([499, 595, 596, 597, 598]), array([500, 876, 880, 607, 390]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([746, 848, 833, 276, 255]), array([ 44, 620, 512, 441, 787]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([934, 191, 248, 722, 119]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([207, 248, 722, 191, 119]), array([278, 685, 112,  16,  99]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([780,  25, 772, 530, 943]), array([499, 595, 596, 597, 598]), array([118, 207, 627, 208, 722]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([541, 972, 448, 845, 759]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([792, 563,   0, 198,  56]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([203, 494, 841, 127, 105]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([539, 797, 706, 503, 579]), array([585, 306, 280,  71, 181]), array([650, 295, 915, 655, 929]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([290, 977,  49, 202, 962]), array([499, 595, 596, 597, 598]), array([500, 876, 880, 607, 390]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([265, 409, 498, 623, 534]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([211, 321, 293, 332, 519]), array([499, 595, 596, 597, 598]), array([771, 604,  67, 324, 213]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([499, 595, 596, 597, 598]), array([816, 790, 504, 605, 735]), array([500, 876, 880, 607, 390])]\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X0c6mJjco_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_list3 = []\n",
        "for i in range(100):\n",
        "  x_list3.append([train_labels[z] for z in list_result3[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EjkizUMc8mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_list2 = []\n",
        "for i in range(100):\n",
        "  x_list3[i]\n",
        "  final_list2.append(Counter(x_list3[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5jC3vruc02e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "38e64839-410a-49a0-8fde-00692061d56f"
      },
      "source": [
        "predictions2 = []\n",
        "for item in final_list2:\n",
        "  predictions2.append(list(item.keys())[0])\n",
        "\n",
        "print(predictions2)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 3, 9, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 7, 1, 7, 7, 7, 7, 4, 7, 7, 7, 3, 9, 7, 7, 7, 7, 4, 7, 9, 7, 7, 7, 7, 9, 7, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 7, 7, 7, 2, 8, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 5, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPxCKig0dSKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "989261c1-08e0-4ea1-fada-75b88e39236d"
      },
      "source": [
        "f=0\n",
        "for i in range(100):\n",
        "  if(predictions2[i]== test_labels[i]):\n",
        "    f=f+1\n",
        "  \n",
        "print(f)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU2ELSy5dh84",
        "colab_type": "text"
      },
      "source": [
        "                          the accuarcy for all the 3 is 16%"
      ]
    }
  ]
}